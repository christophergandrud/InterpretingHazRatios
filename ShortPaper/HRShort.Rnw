%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Showing Estimates from Interactive and Nonlinear Cox Proportional Hazard Models in Political Science
% Christopher Gandrud
% 13 May 2013
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper]{article}
\usepackage{fullpage}
\usepackage[authoryear]{natbib}
\usepackage{setspace}
    \doublespacing
\usepackage{amsmath}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=cyan,
    urlcolor=cyan
}
\usepackage{dcolumn}
\usepackage{tikz}
\usepackage{todonotes}

<<include=FALSE>>=
#### Load Packages ####
library(repmis)

Packages <- c("car", "knitr", "gridExtra", "repmis", "devtools", "MSBVAR", 
              "simPH", "SPIn", "survival", "Zelig", "ggplot2")

LoadandCite(Packages, file = "HRPackages.bib")

#### Load data ####
# Load Carpenter (2002) data 
## The data is included with simPH
data("CarpenterFdaData")

# Load Golub & Steunenberg (2007) data
## The data is included with simPH
data("GolubEUPData")

##### Set Chunk Options ####
opts_chunk$set(fig.align='center', dev='png')
@

\begin{document}

% Title
\title{Showing Estimates for Interactive and Nonlinear Effects from Cox Proportional Hazard Models}
    \author{Christopher Gandrud\footnote{Lecturer in International Relations Yonsei University (\href{mailto:gandrud@yonsei.ac.kr}{gandrud@yonsei.ac.kr}). Thank you to Andreas Beger, Jeffrey Chwieroth, Kelly Kadera, Luke Keele, Meredith Wilf, and participants at the International Studies Association Annual Convention (2013). Note: I wrote the paper with \texttt{knitr} \citep{R-knitr}. The paper can be completely reproduced from source code files available at: \url{https://github.com/christophergandrud/InterpretingHazRatios}.}}

\maketitle

% Abstract
\begin{abstract}

\noindent\emph{Draft Version. Comments welcome.} \\[0.2cm]

The Cox Proportional Hazard (PH) model is a popular tool for political scientists examining cross-unit cross-time data. However, many researchers misspecify their models and poorly communicate uncertainty about their estimates. This is unfortunate because causes of model misspecification--e.g. interactive and nonlinear effects--may be substantively meaningful. Uncertainty about these effects can be difficult to assess because quantities of interest are often on asymmetric and nonlinear scales. Part of the problem has been that available computational tools make it difficult to explore and communicate these effects. This paper improves the way researchers use Cox-type models in two ways. First it advocates using  visually-weighted shortest probability intervals to display simulated quantities of interest. Second, it makes it easy for researchers to use these methods by demonstrating a new R package--\emph{simPH}.

\end{abstract}

\begin{description}
  \item [{\textbf{Keywords:}}] Cox Proportional Hazard models, Event History Analysis, hazard ratios, time-varying, nonlinearity, splines
\end{description}

\vspace{0.3cm}

The Cox \citeyearpar{cox1972} Proportional Hazards (PH) model and Event History Analysis (EHA) in general are increasingly being used to help answer political science questions. A small sample of recent studies using Cox PH and EHA generally includes: \cite{Aleman2011, BuenodeMesquita1999, brooks2005, Gandrud2012, Gates2006, Golub2007, Mccubbins2009, Neumayer2002, Simmons2006}. However, many researchers misspecify their models and poorly communicate uncertainty about their estimates. This is unfortunate because causes of model misspecification--e.g. interactive and nonlinear effects--may be substantively meaningful. Uncertainty about these effects can be difficult to assess because quantities of interest are often on asymmetric and nonlinear scales. Part of the problem has been that available computational tools make it difficult to explore and communicate these effects. 

This article aims to improve the way political scientists use Cox-type models\footnote{I use the term `Cox-type' model to refer to both proportional and nonproportional Cox Hazard models.} in two ways. After laying out previous research on how Cox-type models can be misspecified by ignoring time-varying and non-linear effects, it (a) advocates using shortest probability intervals and visual-weighting to display simulated quantities of interest describing these estimated effects. Then (b) it makes it easy for researchers to use these methods by demonstrating a new R \citep{RLanguage} package--\emph{simPH} \citep{R-simPH}. The package makes it easier to create visually-weighted plots of central or shortest probability intervals of simulated distributions of quantities of interest estimated from Cox-type models. The latter type of interval is often more appropriate for Cox-type model results. Quantities of interest \emph{simPH} can handle include hazard ratios, first differences, relative hazards, marginal effects (when applicable), and hazard rates from linear, linear-interactive, time-varying, and nonlinear coefficients. I use empirical examples to illustrate \emph{simPH}'s capabilities. 

%%%%%% Section: Common issues
\section{Issues in Analyses Using Cox PH Models}

In this section I will briefly review a number of ways that Cox PH models can be misspecified. Misspecification problems include not explicitly modeling time-varying effects and nonlinearities. 

Before discussing these problems let's quickly look at what Cox PH models are. The Cox PH model \citep{cox1972} is a semi-parametric EHA model that allows researchers to examine how specified factors influence the rate of a particular event happening--e.g. a policy is adopted, a government falls, or a war breaks out--at a particular point in time given that the event has not already occurred.\footnote{See Box-Steffensmeier and Jones \citeyearpar[47]{boxsteffensmeier2004} for a discussion of the model's popularity in political science. In general the issues discussed here also apply to Fine and Gray's \citeyearpar{Fine1999} Competing Risks Model, a Cox PH analogue for competing risks EHA. \emph{simPH} currently does not work with competing risks models, though hopefully future versions will.} This rate is commonly referred to as the hazard rate ($h_{i}(t)$).\footnote{Formally: $h_{i}(t) = \lim\limits_{\Delta t \rightarrow 0}\frac{\mathrm{Pr}(t \leq T < t + \Delta t | T \leq t)}{\Delta t}$, where $T$ is the time that an event occurred over the interval $[t,\:\Delta t]$.} The hazard rate for unit $i$ at time $t$ is estimated with the Cox PH model using: 
%
\begin{equation}
    h(t|\mathbf{X}_{i})=h_{0}(t)\mathrm{e}^{(\mathbf{\beta X}_{i})},
\end{equation}
%
where $h_{0}(t)$ is the baseline hazard, i.e. the instantaneous rate of a transition at time $t$ when all of the covariates are zero. $\mathbf{\beta}$ is a vector of coefficients and $\mathbf{X}_{i}$ is the vector of covariates for unit $i$.

We are often interested in how a covariate changes the rate of an event happening. For example, does having a majoritarian electoral system increase the probability of a country adopting a given policy? In general researchers have tried to answer these questions by looking at Cox PH coefficient estimates $\beta$.\footnote{As we will see, like logistic regression, the coefficient is more easily interpreted if we examine its exponent $\mathrm{e}^{\beta}$.} However, only examining single coefficients can lead to significant model misspecification and erroneous substantive interpretation of Cox PH results.

\section{Non-proportional hazards \& Time-varying effects}

One of the most important sources of estimation bias in Cox PH models discussed at length by \cite{Licht2011}, \cite{BoxSteffensmeier2001}, and \cite{boxsteffensmeier2004} is a violation of the proportional hazards assumption (PHA). The PHA is that the hazards of two units experiencing an event are proportional to one another and that this relationship is constant over time. Formally, for the PHA to hold the hazard for units $j$ and $l$ must be:\footnote{This is also the equation for the hazard ratio between $X_{j}$ and $X_{l}$.}
%
\begin{equation}
	\frac{h_{j}(t)}{h_{l}(t)} = \mathrm{e}^{\beta\prime(x_{j} - x_{l})}.
\end{equation}
%
for all points in time. If the PHA is violated and measures are not taken to correct for the violation, then researchers may create biased parameter estimates and statistical tests with lower power \citep{Therneau1990,Keele2010}. Beyond these statistical problems, not adjusting for violations of the PHA can obscure phenomenon that political researchers are interested in studying.

\cite{Licht2011} identifies a number of important political science theories that \emph{expect} the PHA will be violated. She argues that ``the nature of the political processes of learning, institutionalization, strategic developments, and information transmission \ldots are likely to produce frequent violations of the PHA'' \citeyearpar[228]{Licht2011}. For example, building on \cite{Finnemore1998} and \cite{blyth1997} researchers such as \cite{Gandrud2012} have attempted to empirically examine how ideas may affect policy change and diffusion relative to geopolitical and domestic political and economic factors, among others. An important empirically observable difference between ideational theories and others is how they predict the effects of specific factors \emph{change over time relative to others}.

There are a number of widely used tests to examine if the PHA has been violated. See \cite{Grambsch1994}, \cite{BoxSteffensmeier2001}, and \cite{boxsteffensmeier2004} for discussions of various PHA testing methods.\footnote{Many software packages implement versions of these tests. R's \emph{survival} package \citep{R-survival} implements Grambsch and Therneau's \citeyearpar{Grambsch1994} modified Schoenfeld residuals test. This is done with the \texttt{cox.zph} command.} If a covariate is determined to violate the PHA, Box-Steffensmeier and co-authors \citep[see][]{BoxSteffensmeier2003,boxsteffensmeier2004} suggest directly modeling the relationship between the variable and time. This usually entails including an interaction between the variable and some function of time such as the natural logarithm or some exponent.\footnote{The decision to use a particular functional form should be guided by theory and will likely also be influenced by findings in the data.} If $f(t)$ is some function of time then a simple Non-proportional Hazards (NPH) Cox model estimating the hazard rate for unit $i$ with one time-interaction is given by:
%
\begin{equation}
	h_{i}(t|\mathbf{x}_{i})=h_{0}(t)\mathrm{e}^{(\beta_{1}x_{i} + \beta_{2}f(t)x_{i})}.
\end{equation}

Like any other interaction effect \cite[see][]{Brambor2006} extra care should be taken when interpreting the $\beta_{1}$ and $\beta_{2}$ parameter estimates and their associated uncertainty. We cannot simply interpret the effect by looking at $\beta_{1}$ or $\beta_{2}$ in isolation. They need to be combined. Licht argues that post-estimation simulation techniques should be employed to substantively interpret these combined coefficients and the uncertainty surrounding them. Let's briefly look at ways to calculate combined effects. Later in this section we will look at showing our uncertainty about them using simulation techniques. 

Licht describes two methods of calculating the combined effect of a time interaction on the hazard of an event happening in ways that are relatively easy to interpret: (a) first differences and (b) relative hazards. A first difference is the percentage change in the hazard rate at time $t$ between two values of $x$:
%
\begin{equation}
	\%\triangle h_{i}(t) = (e^{(x_{j} - x_{l})(\beta_{1} + \beta{2}f(t))} - 1) * 100.
\end{equation}
% 
Relative hazards\footnote{The term was advanced by \cite{Golub2007}.} are given by:
%
\begin{equation}
	\frac{h_{j}(t)}{h_{l}(t)} = \mathrm{e}^{x_{j}(\beta_{1} + \beta_{2}f(t))},
\end{equation}
%
In this situation the covariate $x_{l}$ is 0. Relative hazards represent the change in the hazard when $x$ is ``switched on'' as it would be when comparing a European Union member to a non-member, for example. As such, relative hazards are a special case of the hazard ratio \citep[231]{Licht2011}, i.e. the expected change in the hazard when $x$ is fitted at a value different from zero compared to when $x$ is zero. Of course the difference between the these quantities of interest is largely cosmetic if $x_{j}$ and $x_{l}$ are set at the same values.
 
\subsection{Non-proportional hazards \& Nonlinear Effects}

Time-varying coefficient effects are not the only cause of PHA violations. Building on \cite{Grambsch1994} and \cite{Therneau2000}, \cite{Keele2010}\footnote{See also \cite{Keele2008} chapter 6.} points out that common diagnostic tests will also indicate PHA violations if the model is misspecified for other reasons. Omitting an important covariate, using a proportional hazards model even if another EHA model is more appropriate, or including a covariate as linear when its effect is actually nonlinear can lead to significant PHA tests. 

Because of this Keele suggests that \emph{before} testing the PHA we should (a) try to make sure that we are not omitting important variables and (b) find the covariates' appropriate functional form, typically using either polynomials or splines.\footnote{See \cite{Keele2008} for a review of methods for identifying nonlinear effects and different spline types.} He demonstrates this in replication studies by adding penalized splines then using a Wald test to examine if the spline estimates have a better fit than their linear counterparts.\footnote{He also suggests that likelihood ratio tests can be used \citeyearpar[195]{Keele2010}.} Many studies using Cox PH models did not test for nonlinearity, but instead jumped straight to testing the PHA, including time-interactions when it was violated. As Keele \citeyearpar{Keele2010} demonstrates that ascribing a time-varying effect to a covariate when in fact the effect varies not over time, but nonlinearly over values of the covariate can have major implications for substantive interpretation of results from Cox PH models.

\section{Show Your Estimation Uncertainty}

How can we effectively examine and communicate both the point estimates of and our uncertainty about quantities of interest from time interactive and nonlinear effects? In this section I advocate a way of showing these results using simulations, shortest probability intervals, and visually-weighted plots.

\subsection{Post Estimation Simulations}

Following \cite{King2000}, Licht advocates post-estimation simulation techniques to make it easier to estimate the uncertainty surrounding quantities of interest for time interactions like first differences and relative hazards.\footnote{See \citet[352-353]{King2000} for a discussion of alternative approaches including fully Bayesian Markov-Chain Monte Carlo techniques and bootstraping. The main difference between these three approaches is how the parameters are drawn.} In both cases we first find the parameter point estimates for $\hat{\beta_{1}}$ and $\hat{\beta_{2}}$ from a NPH Cox model that make up the time-varying effect as well as the parameter covariance estimates. Second, we draw $n$ values of $\beta_{1}$ and $\beta_{2}$ from the multivariate normal distribution with a mean of $\hat{\beta}$ and variance specified by the parameters' estimated covariance. Third, we use these simulated values to calculate a quantity of interest such as the first difference or relative hazard for a range of times $T = [t_{1}\ldots t_{T}]$ as well as specified values of $x_{j}$ and $x_{l}$ (as appropriate). Finally, we plot the results. Using this simulation technique allows us to estimate full time-varying effects, how they change over time, substantively asses the effects, and show the uncertainty surrounding the estimates.

We can easily extend Licht's simulation technique to quantities of interest for other estimated effect types, including nonlinear effects. For example if a nonlinear effect is modeled with a second order polynomial--i.e. $\beta_{1}x_{i} + \beta_{2}x_{i}^{2}$--we can once again draw $n$ simulations from the multivariate normal distribution for both $\beta_{1}$ and $\beta_{2}$. Then we simply calculate quantities of interest for a range of values and plot the results as before. For example, we find the first difference for a second order polynomial with:
%
\begin{equation}
    \%\triangle h_{i}(t) = (\mathrm{e}^{\beta_{1}x_{j-1} + \beta_{2}x_{j-l}^{2}} - 1) * 100,
\end{equation}
%
\noindent where $x_{j-l} = x_{j} - x_{l}$. Note this will not show the effect over time,\footnote{For this we need to estimate the hazard rate.} but over a range of values of $x$.

We can use a similar procedure for splines. Penalized splines \citep{Eilers1996} are a commonly used way of showing more complex nonlinear effects than polynomials allow \cite[see][]{Keele2008}. They are basically ``linear combinations of B-spline basis functions'' \citep[5]{Strasak2009} joined at points in the range of observed values of $x$ called ``knots'' \citep[50]{Keele2008}. A Cox PH model with one penalized spline is given by:
%
\begin{equation}
    h(t|\mathbf{X}_{i})=h_{0}(t)\mathrm{e}^{g(x)},
\end{equation}
%
where $g(x)$ is the penalized spline function. For our post-estimation purposes $g(x)$ is basically a series of linearly combined coefficients such that:
%
\begin{equation}
    g(x) = \beta_{k_{1}}(x)_{1+} + \beta_{k_{2}}(x)_{2+} + \beta_{k_{3}}(x)_{3+} + \ldots + \beta_{k_{n}}(x)_{n+},
\end{equation}  
%
where $k$ are the equally spaced spline knots with values inside of the range of observed $x$ and $n$ is the number of knots. $x_{c+}$ indicates that:
%
\begin{equation}
    (x)_{c+} = 
    \left \{
    \begin{array}{ll}
        x & \quad \text{if} \: k_{c-1} < x \leq k_{c} \\
        x & \quad \text{if} \: x \leq k_{1} \: \text{and} \: k_{c} = k_{1} \\
        x & \quad \text{if} \: x \geq k_{n} \: \text{and} \: k_{c} = k_{n} \\
        0 & \quad \text{otherwise,}
    \end{array}
    \right.
\end{equation}
%
Note, $x$ should be within the observed data. 

We can again draw values of each $\beta_{k_{1}}, \ldots \beta_{k_{n}}$ from the multivariate normal distribution described above. We then use these simulated coefficients to estimates quantities of interest for a range covariate values. For example, the first difference between two values $x_{j}$ and $x_{l}$ is:
%
\begin{equation}
    \%\triangle h_{i}(t) = (\mathrm{e}^{g(x_{j}) - g(x_{l})} - 1) * 100.
\end{equation}
%
Relative hazards and hazard ratios can be calculated by extension. Once we find the simulated quantities of interest, plotting the results is straightforward. 

This post-estimation technique is not limited to time-varying covariates, but can be used for virtually any quantities of interest estimated from Cox-type models including hazard rates and marginal effects for interactions \cite[see][]{Brambor2006}, as well as plain linear effects. 

\subsection{Which Interval to Show?}

If researchers go beyond the usual `train timetable' coefficient tables and graphically show their parameter estimates they usually do so by plotting lines of some measure of central tendency and confidence bands calculated from standard errors over a range of fitted values. Previous work with post-estimation simulations, e.g. \cite{Licht2011}, has mirrored this approach in graphs with a line for the median or mean of the simulation distribution and lines representing the boundaries of a central interval of the distribution. For example, the central 95 percent interval could be represented by lines at the 2.5 and 97.5 percentiles of the distribution.

Many quantities of interest from Cox-type models have asymmetric probability distributions on a nonlinear scale and can therefore be poorly summarized by central intervals. Recall that most quantities of interest are on an exponential scale with a lower bound of 0 or, in the case of first differences, -100. They can have very long thin upper tails. In these cases it can be more useful to look at highest density regions \citep[see][]{Box1973,Hyndman1996}. The underlying idea is that we should be more interested in the set of quantities of interest values with the most probability \cite[120]{Hyndman1996}, rather than an arbitrary central interval. When the simulation has a normal distribution, the highest density region will be equivalent to the central interval with the same percentage of the simulations, e.g. 95 percent. However, when the highest density are at the boundary--for example when many of the simulated relative hazard values are close to 0--then \cite{Liu2013} argue that the highest density region is preferable to the central interval. In these cases ``central intervals can be much longer and have the awkward property [of] cutting off a narrow high-posterior slice that happens to be near the boundary, thus ruling out a part of the distribution that is actually strongly supported by the inference'' \citep[2]{Liu2013}. If this happens \citeauthor{Liu2013} recommend finding the shortest probability interval (SPIn). This is the shortest interval of a particular probability coverage based on the simulations. They find this to be a very efficient way of finding the shortest highest density region for unimodal distributions.

\subsection{Visual Weighting}

Whether graphing a central or shortest probability interval, it is common to use lines to represent the edges on a graph \citep[see][]{Licht2011}. The only other information given to the reader is typically a third line for some measure of central tendency. Some graphs shade the region between the interval's edges. This approach over emphasizes the edges of the interval, the areas of lowest probability. Uniform shading suggest to the reader a uniform distribution between the edges. Both of these characteristics give misleading information about the quantities of interest probability distributions, especially when they are on an exponential scale.

Visual weighting presents a solution to these problems. Hsiang calls visual weight ``the amount of a viewer's attention that a graphical object or display region attracts, based on visual properties'' \cite[3]{Hsiang2012}. More visual weight can be created by using more ``graphical ink" \citep{Tufte2001}. Visual weight is decreased by removing graphical ink. The simplest way to increase or decrease graphical ink with our simulations is to simply plot each point with some transparency. Areas of the distribution with many simulations will be darker. Areas with fewer simulations, often near the edges will be lighter. Plotting each simulated quantity of interest as semi-transparent points allows a researcher to very clearly communicate a quantity of interest's probability distribution. Each point is partially transparent so areas of the chart where the points are darker indicate areas of higher probability as more points are stacked on top of one another. This approach gives more visual weight to areas of higher probability and avoids drawing the reader's attention to the edges of the distribution--the areas of lower probability--in the that way traditional confidence interval lines do.

%%%%%% Section: simPH
\section{\emph{simPH}: Tools for Simulating and Graphing Effects}

One reason that researchers have inconsistently incorporated suggestions to test for and examine time interactions and nonlinearities in their Cox-type models and show their estimation uncertainty is that there has been a lack of computational capabilities to easily do so. In R the \emph{survival} \citep{R-survival} package has functions for testing the proportional hazards assumption. The \emph{survival} and \emph{Zelig} \citep{R-zelig} packages\footnote{\cite{King2000} also developed the \emph{Clarify} package for Stata. It has very similar capabilities as \emph{Zelig}.} can estimate models with splines and interactions. However, their capabilities for graphically showing these estimates and associated uncertainty are very limited. Current capabilities for showing results from splines\footnote{Primarily R's \texttt{termplot} command. See \cite{Keele2008}, chapter 6.} present results in difficult to interpret quantities and without indicating easy to interpret estimation uncertainty. In general the capabilities for showing results from time interactions is very limited. Usually, showing these types of results requires considerable researcher effort to extract estimates from model objects and then devise ways to show them graphically. See for example Licht's code for replicating the time-interaction plots in her paper.\footnote{It is available at: \url{http://hdl.handle.net/1902.1/15633}.} No method allows you to easily plot shortest probability intervals and virtually none effectively use visual weighting.

To solve these problems I am introducing the \emph{simPH} package for R.\footnote{See the Appendix for installation instructions and source code for the following examples.} There are three basic steps to use the package:

\begin{enumerate}
	\item Estimate a Cox-type model using \emph{survival}'s \texttt{coxph} command,
	\item Simulate parameter estimates and calculate the quantities of interest--e.g. relative hazards, first differences, hazard ratios, or hazard rates\footnote{Marginal effects can also be estimated for linear multiplicative interactions.}--using the \emph{simPH} command corresponding to the variable type.\footnote{\texttt{coxsimLinear} can be used for linear, time constant variables, \texttt{coxsimInteract} for linear multiplicative interactions, \texttt{coxsimPoly} for polynomials, \texttt{coxsimSpline} for penalised splines, and \texttt{coxsimtvc} for time-varying coefficients.}
	\item Plot the simulations using the \texttt{simGG} method.\footnote{It uses \emph{ggplot2} \citep{R-ggplot2} and in some cases \texttt{scatter3d} from the \emph{car} packages \citep{R-car} to plot the simulations. Because in most cases \texttt{simGG} returns a \emph{ggplot}--\texttt{gg}--object you can add any aesthetic attributes to the plots that \emph{ggplot2} will allow.} 
\end{enumerate}


The simulation functions follow \cite{King2000} to simulate parameters\footnote{\emph{simPH} uses the \emph{MSVBAR} \citep{R-MSBVAR} R package to draw the simulations.} and calculate a variety of quantities of interest. The user can specify the number of simulations to run per fitted value. The more simulations conducted,\footnote{The \emph{simPH} default is 1,000.} the better picture we get of the probability distributions our parameters are from \citep[349]{King2000}.\footnote{Warning: in some cases, such as with penalised splines, it is easy to ask the program to create more simulations than average desktop computers can easily handle. Therefore the user may need to balance a desire for a clear view of the probability distribution a quantity of interest comes from with what is computationally feasible.} \emph{simPH}'s simulation commands allow the user to keep either the traditional central interval of the simulations' distribution--the middle 95 percent by default--or use the shortest probability interval--also 95 percent by default.\footnote{To tell \emph{simPH} to find the shortest probability interval, with any of \emph{simPH}'s' simulation commands simply set the argument \texttt{spin = TRUE}. This capability was developed from \cite{Liu2013} and the accompanying code in Liu's \citeyearpar{R-SPIn} \emph{SPIn} R package.}  

The \texttt{simGG} plotting method then takes these simulated values and plots them as semi-transparent points\footnote{The default is 90 percent transparent.} along with a smoothing line of a type specified by the user to summarize the distribution's central tendency. By plotting semi-transparent points for each simulated quantity of interest value, \texttt{simGG} automatically visually weights the simulation distribution.

%%%%%% Section: Demonstrations
\section{Empirical Demonstrations}

To illustrate how \emph{simPH} can be used by political scientists, I will use it to replicate key figures and findings from \cite{Licht2011} and \cite{Keele2010}. The purpose of this section is not to verify the results of these studies, but to demonstrate \emph{simPH}'s' accuracy and usefulness.

\subsection{Time-varying Effects}

There is currently no easy way to visually explore time-varying effects. As such \emph{simPH} substantially improves researchers' abilities to effectively show these results. To demonstrate this, I will recreate plots from \cite{Licht2011}.\footnote{As I mentioned earlier, she makes the Stata source code available. \emph{simPH} makes it much easier to implement these methods.} 

She re-examines Golub and Steunenberg's \citeyearpar{Golub2007} analysis of EU directive deliberation. The left panel of Figure \ref{TVCQMV} recreates her figure showing the first difference of a log-time interaction for how the effect of qualified majority voting (QMV) legislative deliberation time changes as the number of days of deliberation increases \citeyearpar[see][236]{Licht2011}.\footnote{Her original figure was not in terms of a percentage difference to make it more comparable to a figure in Golub and Steunenberg's original. I present the results in terms of percentage difference, as the first difference is commonly reported. I have not separated out the pre and post Single European Act time periods for simplicity. There are slight discrepancies in the estimates presented in her figures and those here. These are caused by differences in how the underlying model was estimated in Stata compared to R. Finally, I also examined whether or not nonlinearity functional forms would be better fits than time interactions as per our discussion above. However, I found no evidence of this.} We can clearly see that using QMV increases the probability of passing a directive, early in the deliberation process (almost by 300 percent at about 80 days). But as the deliberation process goes on, the effect decreases and then becomes negative at about 1,000 days. The rate of decrease also levels off after 1,000 days relative to before. The code for how to recreate this graph is in the Appendix. Needless to say, \emph{simPH} enables the graph to be made with only two lines of code, once the model is estimated. This is a significant improvement over Licht's original code. 

\begin{figure}
  \caption{Simulated First Differences for the Effect of Qualified Majority Voting on the Time it Takes to Pass Legislation.}
  \label{TVCQMV}

<<TVCModel, cache=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.width=7, fig.height=4, out.width='0.95\\linewidth'>>=
# Load packages
library(survival)
library(simPH)
library(gridExtra)

# Load Golub & Steunenberg (2007) data
## The data is included with simPH
data("GolubEUPData")

# Create natural log time interactions
Golubtvc <- function(x){
  assign(paste0("l", x), tvc(GolubEUPData, b = x, tvar = "end", tfun = "log"))
}
GolubEUPData$Lcoop <-Golubtvc("coop")
GolubEUPData$Lqmv <- Golubtvc("qmv")
GolubEUPData$Lbacklog <- Golubtvc("backlog")
GolubEUPData$Lcodec <- Golubtvc("codec")
GolubEUPData$Lqmvpostsea <- Golubtvc("qmvpostsea")
GolubEUPData$Lthatcher <- Golubtvc("thatcher")

# Run Model
M1 <- coxph(Surv(begin, end, event) ~
              qmv + qmvpostsea + qmvpostteu +
              coop + codec + eu9 + eu10 + eu12 +
              eu15 + thatcher + agenda + backlog +
              Lqmv + Lqmvpostsea + Lcoop + Lcodec +
              Lthatcher + Lbacklog,
            data = GolubEUPData,
            ties = "efron")

# Create simtvc object for first difference (central interval)
Sim1.1 <- coxsimtvc(obj = M1, b = "qmv", btvc = "Lqmv",
                  qi = "First Difference", Xj = 1,
                  tfun = "log", from = 80, to = 2000,
                  by = 15, ci = 0.95)

# Create simtvc object for first difference (SPIn)
Sim1.2 <- coxsimtvc(obj = M1, b = "qmv", btvc = "Lqmv",
                  qi = "First Difference", Xj = 1,
                  tfun = "log", from = 80, to = 2000,
                  by = 15, ci = 0.95, spin = TRUE)


# Create first difference plots
Plot1.1 <- simGG(Sim1.1, xlab = "\nTime in Days", 
                 title = "Central Interval\n") 
Plot1.2 <- simGG(Sim1.2, ylab = "", xlab = "\nTime in Days",
                 title = "SPIn\n")

# Combine plots
grid.arrange(Plot1.1, Plot1.2, ncol = 2)

######### Save M1 #############
save(M1, file = "M1_ModelObject.RData")

@

{\scriptsize {The figures' points show middle 95 percent of the simulations at each value of FDA staff using either the central or shortest probability interval. \\ The lines summarizing the central tendency of the distribution was created with a generalized additive model for integrated smoothness estimation. \\ As in Licht's original the time period plotted is truncated from 80 to 2,000 to make the estimates more easily interpretable.}}
\end{figure}

Figure \ref{BacklogRH} was also created with \emph{simPH}. It replicates the right panel of Licht's Figure 3 \citeyearpar[][237]{Licht2011}.\footnote{One difference is that she estimates uncertainty from 10 draws of 1000 simulations, whereas Figure \ref{BacklogRH} is based on one draw of 1000 simulations.} This figure demonstrates the effect of different levels of legislative backlog\footnote{i.e. number of directives pending approval} on directive deliberation time from 1200 days after the directive was proposed. The effect shown is also modeled as a log-time interaction. The main conclusion we can draw from this presentation of the log-time interaction is that if a piece of legislation is not passed in the first 1200 or so days from when it was proposed, it is less likely that it will be passed if there is a large legislative backlog \cite[for more details see][236-237]{Licht2011}.  

\begin{figure}
  \caption{Simulated Relative Hazards for the Effect of Different Levels of Legislative Backlog on Directive Deliberation Time}
  \label{BacklogRH}

<<TVCBacklog, echo=FALSE, message=FALSE, warning=FALSE, out.width="9cm", out.height="8cm", cache=TRUE>>=
######### Load cached model object ############
load("M1_ModelObject.RData")
# Create simtvc object for relative hazard
Sim2 <- coxsimtvc(obj = M1, b = "backlog", btvc = "Lbacklog",
                  qi = "Relative Hazard",
                  Xj = seq(40, 200, 40),
                  tfun = "log", from = 1200, to = 2000,
                  by = 10)

# Create relative hazard plot
simGG(Sim2, xlab = "\nTime in Days", leg.name = "Backlogged \n Items")
@
{\scriptsize{The figure's points show the middle 95 percent of the simulations at each point in time and each fitted value. \\ The line summarizing the central tendency of the distribution was created with a generalized additive model for integrated smoothness estimation.}}
\end{figure}

In examples, the simulation probability distributions are not strongly influenced by boundary effects or long upper tails. Because of this the central and shortest probability intervals are largely equivalent. For example, see the right panel of Figure \ref{TVCQMV}. The distributions are also relatively tight, which is indicated by fairly even visual weight across the distributions.

\subsection{Nonlinear effects}

To demonstrate why researchers need to check for and explicitly model nonlinearity \cite{Keele2010} replicated (among others) a study by \cite{Carpenter2002} on the time it takes the US Food and Drug Administration (FDA) to approve a new drug. Using the steps discussed above, he found that that modeling nonlinearity with penalized splines rather than time-varying effects was the more appropriate strategy for dealing with covariates that violated the PHA.  This allowed him to draw conclusions that, for example, the number of FDA review staff increases the likelihood that a drug will be accepted, but that the effect diminishes after a threshold number of staff are assigned. 

Previously it has been difficult to examine and communicate the functional form, magnitude, and uncertainty surrounding spline effects. Coefficient tables are very cumbersome, because a spline fitted effect is estimated using multiple coefficients and standard errors for values of a variable in a given range--demarcated by the knots--on the hazard. Depending on the size of the ranges\footnote{With R's \texttt{pspline} command the range can effectively be adjusted by changing the spline's degrees of freedom.} there can quickly be many more coefficients than can be efficiently presented in a table and understood by a reader. In his results tables, Keele does not show spline coefficients and simply denotes their overall significance with standard significance stars. In R you can plot--and Keele includes replication code to do so--the estimated spline effect over a range of values.\footnote{This is done with the \texttt{termplot} command.} These plots, however, have a number of drawbacks. First, the plots show the log hazard ratio,\footnote{Log hazard ratios for a standard Cox PH model are given by: $\mathrm{log} \left\{\frac{h_{j}(t)}{h_{h}(t)}\right\} = \mathbf{\beta X}_{i}$ \cite[modified from][49]{boxsteffensmeier2004}.} which is not a particularly intuitive quantity to understand and rarely reported in studies using Cox PH. More importantly it plots standard errors, instead of the more widely used central confidence intervals. Casual readers could easily think the uncertainty around the spline estimates is smaller than it really is.\footnote{Remember that the 95 percent confidence interval $CI$ for some point estimate $\hat{\beta}$ and standard error $SE$ is generally found with: $CI = \hat{\beta} \pm 1.96*SE$.} 

\emph{simPH} allows us to estimate quantities that we are more interested in like relative hazards, hazard rates over time, hazard ratios, and first differences for spline fitted effects.\footnote{If you prefer to model nonlinearities with polynomials, it also supports this. Though currently, it only estimates and plots relative hazards for polynomials.} For example, let's simulate the hazard ratios for the effect of an additional drug review staff on the time it takes for a drug to be approved. Figure \ref{Spline1} shows the simulated hazard ratios over the full range of FDA staff per drug trial observed in Carpenter's data.\footnote{The hazard ratios are for where fitted values of $x_{j}$ and $x_{l}$--FDA staff--are one unit apart, i.e. always $x_{j} - x_{l} = 1$.} Not only is this more informative than simply showing you the 12 coefficients and their standard errors that comprise the spline parameter estimates, it is also more informative than the current plotting alternatives in R, namely \texttt{termplot}. 

<<FitKeeleModel, include=FALSE>>=
# Run basic model
# From Keele (2010) replication data. Used to create Table 7.
M2 <- coxph(Surv(acttime, censor) ~  prevgenx + lethal + deathrt1 +
              acutediz + hosp01 + pspline(hospdisc, df = 4) + 
              pspline(hhosleng, df = 4) + mandiz01 + 
              femdiz01 + peddiz01 + orphdum + natreg + vandavg3 + 
              wpnoavg3 + pspline(condavg3, df = 4) + 
              pspline(orderent, df = 4) + pspline(stafcder, df = 4), 
              data = CarpenterFdaData)
@

\begin{figure}
  \caption{Simulated Hazard Ratios for the Effect of FDA Staff on Drug Approval Time: Central 95\% Interval}
  \label{Spline1}
<<Spline1Fig, echo=FALSE, message=FALSE, out.width="9cm", out.height="8cm", cache=TRUE>>=
# Simulated Fitted Values
Sim3 <- coxsimSpline(M2, bspline = "pspline(stafcder, df = 4)", 
                     bdata = CarpenterFdaData$stafcder,
                     qi = "Hazard Ratio",
                     Xj = seq(1100, 1700, by = 10), 
                     Xl = seq(1099, 1699, by = 10))

# Plot simulated values
simGG(Sim3, xlab = "\n Number of FDA Drug Review Staff", 
        title = "Central Interval\n",
        palpha = 0.2)
@  

{\scriptsize {The figure's points show the middle 95 percent of the simulations at each value of FDA staff. \\ The line summarizing the central tendency of the distribution was created with a generalized additive model for integrated smoothness estimation.}}
\end{figure}

\begin{figure}
  \caption{Log Hazard for the Effect of FDA Staff on Drug Approval Time}
  \label{TermPlot}
<<Termplot, echo=FALSE, out.width="9cm", out.height="8cm", cache=TRUE>>=
# Create termplot for stafcder
termplot(M2, term = 17, se = TRUE, rug = TRUE, 
         ylab = "Log Hazard", xlab = "Number of FDA Drug Review Staff")
@
{\scriptsize {Dashed lines indicate standard errors.}}
\end{figure}

Figure \ref{TermPlot} uses \texttt{termplot} to plot the log hazard of the effect and the standard errors. Apart from showing related but different quantities of interest, Figure \ref{TermPlot} gives us less useful information than Figure \ref{Spline1} about the probability distribution that the parameter is from. By only describing the distribution of the parameter with the mean value and standard error, Figure \ref{TermPlot} makes it difficult for us to know what the distribution is. Figure \ref{Spline1} simulates this distribution and shows it directly.

We can see in Figure \ref{Spline1} that the simulated values are concentrated near the bottom of the distribution. Following \citeauthor{Liu2013} we may find it useful to show not the central 95 percent interval, but the shortest 95 percent probability interval. Figure \ref{Spline2} shows this interval. Using the shortest 95 percent probability interval indicates that there is actually a higher probability that the hazard ratio is 1--i.e. no effect--than the central 95 percent interval for all but the highest quartile or so of FDA drug review staff. It also deemphasizes the higher hazard ratio values, where there is a low concentration of the probability. In both figures \ref{Spline1} and \ref{Spline2} the visual weighting draws readers' attention to the lower part of the distribution where the model estimates there is a higher probability that the hazard ratio will be. 

\begin{figure}
  \caption{Simulated Hazard Ratios for the Effect of FDA Staff on Drug Approval Time: Shortest 95\% Probability Interval}
  \label{Spline2}
<<Spline2Fig, echo=FALSE, message=FALSE, out.width="9cm", out.height="8cm", cache=TRUE>>=
# Simulated Fitted Values: shortest probability interval
Sim4 <- coxsimSpline(M2, bspline = "pspline(stafcder, df = 4)", 
                     bdata = CarpenterFdaData$stafcder,
                     qi = "Hazard Ratio",
                     Xj = seq(1100, 1700, by = 10), 
                     Xl = seq(1099, 1699, by = 10), 
                     spin = TRUE)

# Plot simulated values
SimPlot <- simGG(Sim4, xlab = "\n Number of FDA Drug Review Staff",
                title = "SPIn\n", 
                palpha = 0.2)

# Place on the same scale as the central interval figure
library(ggplot2)

SimPlot + scale_y_continuous(breaks = c(0, 20, 40, 60), limits = c(0, 60))
@  

{\scriptsize {The figure's points show the shortest 90 percent probability interval at each value of FDA staff. \\ The line summarizing the central tendency of the distribution was created with a generalized additive model for integrated smoothness estimation.}}
\end{figure}

\section{Conclusion}

In this paper I have reviewed a number of important insights about avoiding Cox PH model misspecification. Looking out for and properly estimating the interactions and nonlinearities that can cause biased estimates is crucial for political scientists. They can actually be factors that we are interested in studying. Until now the tools for fully exploring them, including their associated uncertainty, have been lacking. So, this paper has illustrated a new R package--\emph{simPH}--that makes it considerably easier to effectively explore and present quantities of interest. The paper has also argued for and demonstrated the usefulness of visual weighting and shortest probability intervals for understanding Cox-type Hazard models. \emph{simPH} fully supports these methods. The package is freely available for researchers to download using instructions in the Appendix.

\section*{Appendix}

\subsection*{Installing \emph{simPH}}

\emph{simPH} is currently available for download from GitHub.\footnote{\url{https://github.com/}} To install it in R type the following code into your R Console:

<<eval=FALSE>>=
devtools::install_github("simPH", "christophergandrud")
@

\noindent You need to have the \emph{devtools} package \citep{R-devtools} set up to install \emph{simPH}.

For more information about \emph{simPH} see: \url{http://christophergandrud.github.io/simPH/}. Please report any bugs to \url{https://github.com/christophergandrud/simPH/issues}.

\subsection*{Source Code}

To help you implement the functions in \emph{simPH} I've included the R source code I used to create the figures in this article. 

\subsubsection*{Figure \ref{TVCQMV}}

<<TVCQMVCode, eval=FALSE, tidy=FALSE>>=
# Load packages
library(survival)
library(simPH)
library(gridExtra)

# Load Golub & Steunenberg (2007) data
## The data is included with simPH
data("GolubEUPData")

# Create natural log time interactions
Golubtvc <- function(x){
  assign(paste0("l", x), tvc(GolubEUPData, b = x, tvar = "end", tfun = "log"))
}
GolubEUPData$Lcoop <-Golubtvc("coop")
GolubEUPData$Lqmv <- Golubtvc("qmv")
GolubEUPData$Lbacklog <- Golubtvc("backlog")
GolubEUPData$Lcodec <- Golubtvc("codec")
GolubEUPData$Lqmvpostsea <- Golubtvc("qmvpostsea")
GolubEUPData$Lthatcher <- Golubtvc("thatcher")

# Run Model
M1 <- coxph(Surv(begin, end, event) ~
              qmv + qmvpostsea + qmvpostteu +
              coop + codec + eu9 + eu10 + eu12 +
              eu15 + thatcher + agenda + backlog +
              Lqmv + Lqmvpostsea + Lcoop + Lcodec +
              Lthatcher + Lbacklog,
            data = GolubEUPData,
            ties = "efron")

# Create simtvc object for first difference (central interval)
Sim1.1 <- coxsimtvc(obj = M1, b = "qmv", btvc = "Lqmv",
                  qi = "First Difference", Xj = 1,
                  tfun = "log", from = 80, to = 2000,
                  by = 15, ci = 0.95)

# Create simtvc object for first difference (SPIn)
Sim1.2 <- coxsimtvc(obj = M1, b = "qmv", btvc = "Lqmv",
                  qi = "First Difference", Xj = 1,
                  tfun = "log", from = 80, to = 2000,
                  by = 15, ci = 0.95, spin = TRUE)


# Create first difference plots
Plot1.1 <- simGG(Sim1.1, xlab = "\nTime in Days", 
                 title = "Central Interval") 
Plot1.2 <- simGG(Sim1.2, ylab = "", xlab = "\nTime in Days",
                 title = "Shortest Probability Interval")

# Combine plots
grid.arrange(Plot1.1, Plot1.2, ncol = 2)
@

\subsubsection*{Figure \ref{BacklogRH}}
<<BacklogCode, eval=FALSE, tidy=FALSE>>=
## Uses fitted model object M2 from above.

# Create simtvc object for relative hazard
Sim2 <- coxsimtvc(obj = M1, b = "backlog", btvc = "Lbacklog",
                  qi = "Relative Hazard",
                  Xj = seq(40, 200, 40),
                  tfun = "log", from = 1200, to = 2000,
                  by = 10, ci = 0.95)

# Create relative hazard plot
simGG(Sim2, xlab = "\nTime in Days", leg.name = "Backlogged \n Items")
@

\subsubsection*{Figure \ref{Spline1}}

<<Spline1Code, eval=FALSE, tidy=FALSE>>=
# Load Carpenter (2002) data 
# The data is included with simPH
data("CarpenterFdaData")

# Run basic model
# From Keele (2010) replication data. Used to create Table 7.
M2 <- coxph(Surv(acttime, censor) ~  prevgenx + lethal + deathrt1 +
              acutediz + hosp01 + pspline(hospdisc, df = 4) + 
              pspline(hhosleng, df = 4) + mandiz01 + 
              femdiz01 + peddiz01 + orphdum + natreg + vandavg3 + 
              wpnoavg3 + pspline(condavg3, df = 4) + 
              pspline(orderent, df = 4) + pspline(stafcder, df = 4), 
              data = CarpenterFdaData)

# Simulated Fitted Values of stafcder
## stafcder is the number of FDA drug review staff
Sim3 <- coxsimSpline(M2, bspline = "pspline(stafcder, df = 4)", 
                     bdata = CarpenterFdaData$stafcder,
                     qi = "Hazard Ratio",
                     Xj = seq(1100, 1700, by = 10), 
                     Xl = seq(1099, 1699, by = 10))

# Plot simulated Hazard Ratios
simGG(Sim3, xlab = "\nNumber of FDA Drug Review Staff ", 
        title = "Central Interval\n",
        palpha = 0.2)
@

\subsubsection*{Figure \ref{TermPlot}}

<<TermPlot, eval=FALSE, tidy=FALSE>>=
# Create termplot for stafcder
termplot(M2, term = 17, se = TRUE, rug = TRUE, ylab = "Log Hazard", 
         xlab = "Number of FDA Drug Review Staff")
@

\subsubsection*{Figure \ref{CentProbSpline}}

<<Spline2Code, eval=FALSE, tidy=FALSE>>=
# Simulated Fitted Values: shortest probability interval
Sim4 <- coxsimSpline(M2, bspline = "pspline(stafcder, df = 4)", 
                     bdata = CarpenterFdaData$stafcder,
                     qi = "Hazard Ratio",
                     Xj = seq(1100, 1700, by = 10), 
                     Xl = seq(1099, 1699, by = 10), 
                     spin = TRUE)

# Plot simulated values
SimPlot <- simGG(Sim4, xlab = "\n Number of FDA Drug Review Staff", 
                title = "SPIn\n", 
                palpha = 0.2)

# Place on the same scale as the central interval figure
library(ggplot2)

SimPlot + scale_y_continuous(breaks = c(0, 20, 40, 60), 
            limits = c(0, 60))

@  

% Bibliography
\bibliographystyle{apsr}
\bibliography{HRBibliography,HRPackages}

\end{document}





